{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad1d1f2b-f6e7-4d4a-8685-19f69941479c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# (optional) force single GPU to avoid NCCL issues; set *before* importing torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "!pip install -q --upgrade accelerate transformers datasets scikit-learn pandas matplotlib evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d2a805-ed5e-4db0-83e2-433c35b42aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 16716, 'validation': 2090, 'test': 2090}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def load_ade_clean(seed=SEED):\n",
    "    # HF ADE classification config\n",
    "    ds = load_dataset(\"ade_corpus_v2\", \"Ade_corpus_v2_classification\")\n",
    "\n",
    "    # unify to one dataframe and keep only text/label\n",
    "    parts = []\n",
    "    for split in ds.keys():\n",
    "        df = ds[split].to_pandas()[[\"text\",\"label\"]]\n",
    "        parts.append(df)\n",
    "    df_all = pd.concat(parts, ignore_index=True).dropna(subset=[\"text\",\"label\"]).reset_index(drop=True)\n",
    "\n",
    "    # collapse exact duplicate texts; binary label by majority vote\n",
    "    df_clean = (\n",
    "        df_all.groupby(\"text\")[\"label\"]\n",
    "              .agg(lambda x: int(round(x.mean())))\n",
    "              .reset_index()\n",
    "    )\n",
    "\n",
    "    # 80/10/10 stratified split\n",
    "    X = df_clean[\"text\"].tolist()\n",
    "    y = df_clean[\"label\"].tolist()\n",
    "    X_tr, X_tmp, y_tr, y_tmp = train_test_split(X, y, test_size=0.20, random_state=seed, stratify=y)\n",
    "    X_va, X_te, y_va, y_te   = train_test_split(X_tmp, y_tmp, test_size=0.50, random_state=seed, stratify=y_tmp)\n",
    "\n",
    "    return DatasetDict({\n",
    "        \"train\":      Dataset.from_dict({\"text\": X_tr, \"label\": y_tr}),\n",
    "        \"validation\": Dataset.from_dict({\"text\": X_va, \"label\": y_va}),\n",
    "        \"test\":       Dataset.from_dict({\"text\": X_te, \"label\": y_te}),\n",
    "    })\n",
    "\n",
    "dataset = load_ade_clean()\n",
    "{k: len(dataset[k]) for k in dataset}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67430751-734b-4333-9cc4-afb9fe5adfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ∩ val : 0\n",
      "train ∩ test: 0\n",
      "val   ∩ test: 0\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "def _H(texts): \n",
    "    return set(hashlib.md5(t.encode(\"utf-8\",\"ignore\")).hexdigest() for t in texts)\n",
    "\n",
    "htr, hva, hte = _H(dataset[\"train\"][\"text\"]), _H(dataset[\"validation\"][\"text\"]), _H(dataset[\"test\"][\"text\"])\n",
    "print(\"train ∩ val :\", len(htr & hva))\n",
    "print(\"train ∩ test:\", len(htr & hte))\n",
    "print(\"val   ∩ test:\", len(hva & hte))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb543822-9823-4d74-92ff-38fe98cf640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, json\n",
    "from typing import Dict, Any\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, f1_score\n",
    "\n",
    "# ---------- TF-IDF + LinearSVM (with calibration) ----------\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def run_tfidf_baseline(dataset, name=\"TF-IDF + LinearSVM\"):\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=50_000, min_df=2)\n",
    "    base  = LinearSVC()\n",
    "    clf   = CalibratedClassifierCV(base, method=\"sigmoid\", cv=3)  # gives probs → AUC, thresholding\n",
    "\n",
    "    pipe = Pipeline([(\"tfidf\", tfidf), (\"clf\", clf)])\n",
    "    Xtr, ytr = dataset[\"train\"][\"text\"],      np.array(dataset[\"train\"][\"label\"])\n",
    "    Xva, yva = dataset[\"validation\"][\"text\"], np.array(dataset[\"validation\"][\"label\"])\n",
    "    Xte, yte = dataset[\"test\"][\"text\"],       np.array(dataset[\"test\"][\"label\"])\n",
    "\n",
    "    pipe.fit(Xtr, ytr)\n",
    "\n",
    "    # tune threshold on validation for best F1\n",
    "    p_va = pipe.predict_proba(Xva)[:,1]\n",
    "    ths  = np.linspace(0.2, 0.8, 61)\n",
    "    best_t, _ = max(((t, f1_score(yva, (p_va>=t).astype(int))) for t in ths), key=lambda x: x[1])\n",
    "\n",
    "    # test metrics at that threshold\n",
    "    p_te = pipe.predict_proba(Xte)[:,1]\n",
    "    yhat = (p_te >= best_t).astype(int)\n",
    "\n",
    "    acc = accuracy_score(yte, yhat)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(yte, yhat, average=\"binary\", zero_division=0)\n",
    "    auc = roc_auc_score(yte, p_te)\n",
    "    cm  = confusion_matrix(yte, yhat).tolist()\n",
    "\n",
    "    return {\"Model\": name, \"ThresholdUsed\": round(float(best_t),2),\n",
    "            \"Test_Accuracy\": float(acc), \"Test_Precision\": float(pr),\n",
    "            \"Test_Recall\": float(rc), \"Test_F1\": float(f1), \"Test_AUC\": float(auc),\n",
    "            \"Checkpoint\": \"(sklearn pipeline)\"}\n",
    "\n",
    "# ---------- Transformer trainer with tuned recipe ----------\n",
    "import torch\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          Trainer, TrainingArguments, EarlyStoppingCallback)\n",
    "\n",
    "def run_transformer(model_id: str, friendly_name: str,\n",
    "                    batch=12, epochs=5, lr=1e-5, sched=\"linear\", warmup_ratio=0.06,\n",
    "                    max_len=512, patience=3) -> Dict[str,Any]:\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    def tok_fn(ex): return tok(ex[\"text\"], truncation=True, max_length=max_len)\n",
    "    dtr = dataset[\"train\"].map(tok_fn, batched=True)\n",
    "    dva = dataset[\"validation\"].map(tok_fn, batched=True)\n",
    "    dte = dataset[\"test\"].map(tok_fn, batched=True)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"/workspace/ade-project/outputs/{friendly_name.replace(' ','_')}\",\n",
    "        per_device_train_batch_size=batch,\n",
    "        per_device_eval_batch_size=batch,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=lr, lr_scheduler_type=sched, warmup_ratio=warmup_ratio,\n",
    "        num_train_epochs=epochs,\n",
    "        eval_strategy=\"steps\", eval_steps=200,\n",
    "        logging_steps=50, save_steps=200, save_total_limit=2,\n",
    "        load_best_model_at_end=True, metric_for_best_model=\"f1\",\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = logits.argmax(axis=1)\n",
    "        # AUC from 2-logit difference\n",
    "        probs = 1/(1+np.exp(-(logits[:,1]-logits[:,0])))\n",
    "        pr, rc, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\", zero_division=0)\n",
    "        try: auc = roc_auc_score(labels, probs)\n",
    "        except: auc = float(\"nan\")\n",
    "        acc = (preds==labels).mean()\n",
    "        return {\"accuracy\": acc, \"precision\": pr, \"recall\": rc, \"f1\": f1, \"auc\": auc}\n",
    "\n",
    "    tr = Trainer(model=model, args=args, train_dataset=dtr, eval_dataset=dva,\n",
    "                 tokenizer=tok, compute_metrics=compute_metrics,\n",
    "                 callbacks=[EarlyStoppingCallback(early_stopping_patience=patience)])\n",
    "    tr.train()\n",
    "\n",
    "    # threshold on validation\n",
    "    pred_va = tr.predict(dva)\n",
    "    p_va = 1/(1+np.exp(-(pred_va.predictions[:,1] - pred_va.predictions[:,0])))\n",
    "    y_va = np.array(pred_va.label_ids)\n",
    "    ths  = np.linspace(0.2, 0.8, 61)\n",
    "    best_t, _ = max(((t, f1_score(y_va, (p_va>=t).astype(int))) for t in ths), key=lambda x: x[1])\n",
    "\n",
    "    # test\n",
    "    pred_te = tr.predict(dte)\n",
    "    p_te = 1/(1+np.exp(-(pred_te.predictions[:,1] - pred_te.predictions[:,0])))\n",
    "    y_te = np.array(pred_te.label_ids)\n",
    "    yhat = (p_te >= best_t).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_te, yhat)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(y_te, yhat, average=\"binary\", zero_division=0)\n",
    "    auc = roc_auc_score(y_te, p_te)\n",
    "    ckpt_path = args.output_dir.replace(\"/outputs/\",\"/models/\")  # simple path tag\n",
    "    model.save_pretrained(ckpt_path); tok.save_pretrained(ckpt_path)\n",
    "\n",
    "    return {\"Model\": f\"{friendly_name} (tuned recipe)\", \"ThresholdUsed\": round(float(best_t),2),\n",
    "            \"Test_Accuracy\": float(acc), \"Test_Precision\": float(pr),\n",
    "            \"Test_Recall\": float(rc), \"Test_F1\": float(f1), \"Test_AUC\": float(auc),\n",
    "            \"Checkpoint\": ckpt_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa9caaf-0473-42f5-a299-b37b6a59e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "715dba07-d395-49c4-a8b7-c0417fb28edd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9160f9c754b644739b4e73b8766ee1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16716 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de4c7b905f542c1ac2bfeeb54951c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2090 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a447a09bc6464e82a89b11804c8986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2090 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1126/526417413.py:83: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  tr = Trainer(model=model, args=args, train_dataset=dtr, eval_dataset=dva,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4200' max='6965' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4200/6965 04:08 < 02:43, 16.89 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.464900</td>\n",
       "      <td>0.418735</td>\n",
       "      <td>0.795694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.826919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.261400</td>\n",
       "      <td>0.203061</td>\n",
       "      <td>0.925359</td>\n",
       "      <td>0.824940</td>\n",
       "      <td>0.805621</td>\n",
       "      <td>0.815166</td>\n",
       "      <td>0.961939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.224300</td>\n",
       "      <td>0.166956</td>\n",
       "      <td>0.938756</td>\n",
       "      <td>0.821505</td>\n",
       "      <td>0.894614</td>\n",
       "      <td>0.856502</td>\n",
       "      <td>0.973449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.174231</td>\n",
       "      <td>0.948804</td>\n",
       "      <td>0.892157</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.871856</td>\n",
       "      <td>0.978883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.250100</td>\n",
       "      <td>0.185410</td>\n",
       "      <td>0.932057</td>\n",
       "      <td>0.933131</td>\n",
       "      <td>0.718970</td>\n",
       "      <td>0.812169</td>\n",
       "      <td>0.979962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.142700</td>\n",
       "      <td>0.196622</td>\n",
       "      <td>0.944498</td>\n",
       "      <td>0.827368</td>\n",
       "      <td>0.920375</td>\n",
       "      <td>0.871397</td>\n",
       "      <td>0.981059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.191100</td>\n",
       "      <td>0.150899</td>\n",
       "      <td>0.952632</td>\n",
       "      <td>0.871041</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.983535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.146700</td>\n",
       "      <td>0.182148</td>\n",
       "      <td>0.953589</td>\n",
       "      <td>0.854077</td>\n",
       "      <td>0.932084</td>\n",
       "      <td>0.891377</td>\n",
       "      <td>0.982664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.165100</td>\n",
       "      <td>0.186311</td>\n",
       "      <td>0.947847</td>\n",
       "      <td>0.832636</td>\n",
       "      <td>0.932084</td>\n",
       "      <td>0.879558</td>\n",
       "      <td>0.984042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.088700</td>\n",
       "      <td>0.178465</td>\n",
       "      <td>0.954067</td>\n",
       "      <td>0.855914</td>\n",
       "      <td>0.932084</td>\n",
       "      <td>0.892377</td>\n",
       "      <td>0.984313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.157100</td>\n",
       "      <td>0.163621</td>\n",
       "      <td>0.955502</td>\n",
       "      <td>0.847917</td>\n",
       "      <td>0.953162</td>\n",
       "      <td>0.897464</td>\n",
       "      <td>0.983726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.154500</td>\n",
       "      <td>0.164348</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.904878</td>\n",
       "      <td>0.868852</td>\n",
       "      <td>0.886499</td>\n",
       "      <td>0.984537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.163300</td>\n",
       "      <td>0.162175</td>\n",
       "      <td>0.958852</td>\n",
       "      <td>0.865096</td>\n",
       "      <td>0.946136</td>\n",
       "      <td>0.903803</td>\n",
       "      <td>0.986222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.110200</td>\n",
       "      <td>0.165561</td>\n",
       "      <td>0.957416</td>\n",
       "      <td>0.853556</td>\n",
       "      <td>0.955504</td>\n",
       "      <td>0.901657</td>\n",
       "      <td>0.985951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.072400</td>\n",
       "      <td>0.181170</td>\n",
       "      <td>0.960287</td>\n",
       "      <td>0.870690</td>\n",
       "      <td>0.946136</td>\n",
       "      <td>0.906846</td>\n",
       "      <td>0.984823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.094900</td>\n",
       "      <td>0.185384</td>\n",
       "      <td>0.958373</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.936768</td>\n",
       "      <td>0.901917</td>\n",
       "      <td>0.984994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.105800</td>\n",
       "      <td>0.162545</td>\n",
       "      <td>0.959809</td>\n",
       "      <td>0.885393</td>\n",
       "      <td>0.922717</td>\n",
       "      <td>0.903670</td>\n",
       "      <td>0.986344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.104000</td>\n",
       "      <td>0.180055</td>\n",
       "      <td>0.964115</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.936768</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.983684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.113800</td>\n",
       "      <td>0.195949</td>\n",
       "      <td>0.955981</td>\n",
       "      <td>0.886836</td>\n",
       "      <td>0.899297</td>\n",
       "      <td>0.893023</td>\n",
       "      <td>0.984832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.144400</td>\n",
       "      <td>0.201742</td>\n",
       "      <td>0.955502</td>\n",
       "      <td>0.863043</td>\n",
       "      <td>0.929742</td>\n",
       "      <td>0.895152</td>\n",
       "      <td>0.985554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>0.209907</td>\n",
       "      <td>0.959809</td>\n",
       "      <td>0.878587</td>\n",
       "      <td>0.932084</td>\n",
       "      <td>0.904545</td>\n",
       "      <td>0.983346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8814c0028d0492c8a384f83d41cef1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16716 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f46810ce7bf427e9c212f71842e784a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2090 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5956b80da4540b8be3f957b0a95ef0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2090 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1126/526417413.py:83: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  tr = Trainer(model=model, args=args, train_dataset=dtr, eval_dataset=dva,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2400' max='6965' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2400/6965 02:16 < 04:19, 17.57 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.423700</td>\n",
       "      <td>0.377722</td>\n",
       "      <td>0.846890</td>\n",
       "      <td>0.824242</td>\n",
       "      <td>0.318501</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.863566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.214900</td>\n",
       "      <td>0.248077</td>\n",
       "      <td>0.906220</td>\n",
       "      <td>0.700173</td>\n",
       "      <td>0.946136</td>\n",
       "      <td>0.804781</td>\n",
       "      <td>0.969541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.273900</td>\n",
       "      <td>0.184762</td>\n",
       "      <td>0.939234</td>\n",
       "      <td>0.842466</td>\n",
       "      <td>0.864169</td>\n",
       "      <td>0.853179</td>\n",
       "      <td>0.976595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.236700</td>\n",
       "      <td>0.158270</td>\n",
       "      <td>0.949761</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.982449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.216900</td>\n",
       "      <td>0.150996</td>\n",
       "      <td>0.951675</td>\n",
       "      <td>0.867117</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.884041</td>\n",
       "      <td>0.983685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.177400</td>\n",
       "      <td>0.186293</td>\n",
       "      <td>0.947847</td>\n",
       "      <td>0.841202</td>\n",
       "      <td>0.918033</td>\n",
       "      <td>0.877940</td>\n",
       "      <td>0.986089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.232000</td>\n",
       "      <td>0.145617</td>\n",
       "      <td>0.952153</td>\n",
       "      <td>0.867416</td>\n",
       "      <td>0.903981</td>\n",
       "      <td>0.885321</td>\n",
       "      <td>0.986439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.197398</td>\n",
       "      <td>0.953110</td>\n",
       "      <td>0.871332</td>\n",
       "      <td>0.903981</td>\n",
       "      <td>0.887356</td>\n",
       "      <td>0.985695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.156000</td>\n",
       "      <td>0.155964</td>\n",
       "      <td>0.958373</td>\n",
       "      <td>0.895349</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.898483</td>\n",
       "      <td>0.986942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.102300</td>\n",
       "      <td>0.170629</td>\n",
       "      <td>0.958373</td>\n",
       "      <td>0.902844</td>\n",
       "      <td>0.892272</td>\n",
       "      <td>0.897527</td>\n",
       "      <td>0.987122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.136700</td>\n",
       "      <td>0.204217</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.815109</td>\n",
       "      <td>0.960187</td>\n",
       "      <td>0.881720</td>\n",
       "      <td>0.987146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.148800</td>\n",
       "      <td>0.177070</td>\n",
       "      <td>0.955024</td>\n",
       "      <td>0.849057</td>\n",
       "      <td>0.948478</td>\n",
       "      <td>0.896018</td>\n",
       "      <td>0.985787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b556691afa044e88b7784c0a1aba9940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16716 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d98b74d7be4ce386433ae3c6ee0838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2090 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99de25773ba14b2389a797be8bdc5b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2090 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1126/526417413.py:83: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  tr = Trainer(model=model, args=args, train_dataset=dtr, eval_dataset=dva,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1400' max='6965' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1400/6965 01:20 < 05:19, 17.41 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.521800</td>\n",
       "      <td>0.481337</td>\n",
       "      <td>0.795694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.743760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.317200</td>\n",
       "      <td>0.262418</td>\n",
       "      <td>0.902392</td>\n",
       "      <td>0.826979</td>\n",
       "      <td>0.660422</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.939355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.236000</td>\n",
       "      <td>0.226111</td>\n",
       "      <td>0.902871</td>\n",
       "      <td>0.712121</td>\n",
       "      <td>0.880562</td>\n",
       "      <td>0.787435</td>\n",
       "      <td>0.955750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.256700</td>\n",
       "      <td>0.179940</td>\n",
       "      <td>0.931579</td>\n",
       "      <td>0.828704</td>\n",
       "      <td>0.838407</td>\n",
       "      <td>0.833527</td>\n",
       "      <td>0.966110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.275400</td>\n",
       "      <td>0.216529</td>\n",
       "      <td>0.920574</td>\n",
       "      <td>0.882698</td>\n",
       "      <td>0.704918</td>\n",
       "      <td>0.783854</td>\n",
       "      <td>0.967707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.194800</td>\n",
       "      <td>0.244195</td>\n",
       "      <td>0.909569</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.955504</td>\n",
       "      <td>0.811940</td>\n",
       "      <td>0.974005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.222100</td>\n",
       "      <td>0.204033</td>\n",
       "      <td>0.931579</td>\n",
       "      <td>0.901130</td>\n",
       "      <td>0.747073</td>\n",
       "      <td>0.816901</td>\n",
       "      <td>0.973800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>ThresholdUsed</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Test_Precision</th>\n",
       "      <th>Test_Recall</th>\n",
       "      <th>Test_F1</th>\n",
       "      <th>Test_AUC</th>\n",
       "      <th>Checkpoint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BioBERT (tuned recipe)</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.953589</td>\n",
       "      <td>0.873303</td>\n",
       "      <td>0.903981</td>\n",
       "      <td>0.888377</td>\n",
       "      <td>0.983821</td>\n",
       "      <td>/workspace/ade-project/models/BioBERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PubMedBERT (tuned recipe)</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.953110</td>\n",
       "      <td>0.896386</td>\n",
       "      <td>0.871194</td>\n",
       "      <td>0.883610</td>\n",
       "      <td>0.985381</td>\n",
       "      <td>/workspace/ade-project/models/PubMedBERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BERT-base (tuned recipe)</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.928708</td>\n",
       "      <td>0.798283</td>\n",
       "      <td>0.871194</td>\n",
       "      <td>0.833147</td>\n",
       "      <td>0.967685</td>\n",
       "      <td>/workspace/ade-project/models/BERT-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TF-IDF + LinearSVM</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.886124</td>\n",
       "      <td>0.710468</td>\n",
       "      <td>0.747073</td>\n",
       "      <td>0.728311</td>\n",
       "      <td>0.929341</td>\n",
       "      <td>(sklearn pipeline)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model  ThresholdUsed  Test_Accuracy  Test_Precision  \\\n",
       "0     BioBERT (tuned recipe)           0.32       0.953589        0.873303   \n",
       "1  PubMedBERT (tuned recipe)           0.68       0.953110        0.896386   \n",
       "2   BERT-base (tuned recipe)           0.40       0.928708        0.798283   \n",
       "3         TF-IDF + LinearSVM           0.36       0.886124        0.710468   \n",
       "\n",
       "   Test_Recall   Test_F1  Test_AUC                                Checkpoint  \n",
       "0     0.903981  0.888377  0.983821     /workspace/ade-project/models/BioBERT  \n",
       "1     0.871194  0.883610  0.985381  /workspace/ade-project/models/PubMedBERT  \n",
       "2     0.871194  0.833147  0.967685   /workspace/ade-project/models/BERT-base  \n",
       "3     0.747073  0.728311  0.929341                        (sklearn pipeline)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /workspace/ade-project/outputs/comparison_tuned_recipe.csv\n",
      "Saved: /workspace/ade-project/outputs/comparison_tuned_recipe.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "rows.append(run_transformer(\"dmis-lab/biobert-base-cased-v1.2\", \"BioBERT\"))\n",
    "rows.append(run_transformer(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\", \"PubMedBERT\"))\n",
    "rows.append(run_transformer(\"bert-base-uncased\", \"BERT-base\"))\n",
    "rows.append(run_tfidf_baseline(dataset))\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "display(df)\n",
    "\n",
    "OUT = \"/workspace/ade-project/outputs\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "df.to_csv(f\"{OUT}/comparison_tuned_recipe.csv\", index=False)\n",
    "with open(f\"{OUT}/comparison_tuned_recipe.json\",\"w\") as f: json.dump(rows, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", f\"{OUT}/comparison_tuned_recipe.csv\")\n",
    "print(\"Saved:\", f\"{OUT}/comparison_tuned_recipe.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75ca4ece-32c6-499b-ac3e-2f9b9b69dff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0462826d793748b388317ec964fc6686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/331 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114f5dc11fae496bbc3546c213e80f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3673ea5b7fe4d71b7472c89a73e4958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.jsonl: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc6333e7e3b46bdb72ee9a902b1766a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/17637 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76eeec30a618442cbd5bcea1bddd4e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5879 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"SetFit/ade_corpus_v2_classification\")  # train/test ready\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1820348d-b558-4075-865d-294ae469d676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External test size: 5879\n",
      "Scoring external SetFit — BioBERT (tuned recipe) @ thr=0.32 …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b583b58bc84a8fb0f7fa830322a49b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5879 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1126/4189969397.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  tr = Trainer(model=mdl, tokenizer=tok)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring external SetFit — PubMedBERT (tuned recipe) @ thr=0.68 …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582546b6fb2d4b9599ff8039296532c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5879 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1126/4189969397.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  tr = Trainer(model=mdl, tokenizer=tok)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring external SetFit — BERT-base (tuned recipe) @ thr=0.4 …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9e7584ed6046b5bf52a1aae4d2f74f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5879 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1126/4189969397.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  tr = Trainer(model=mdl, tokenizer=tok)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>ThresholdUsed</th>\n",
       "      <th>Ext_Accuracy</th>\n",
       "      <th>Ext_Precision</th>\n",
       "      <th>Ext_Recall</th>\n",
       "      <th>Ext_F1</th>\n",
       "      <th>Ext_AUC</th>\n",
       "      <th>Checkpoint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BioBERT (tuned recipe)</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.981800</td>\n",
       "      <td>0.961742</td>\n",
       "      <td>0.974940</td>\n",
       "      <td>0.968296</td>\n",
       "      <td>0.996484</td>\n",
       "      <td>/workspace/ade-project/models/BioBERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PubMedBERT (tuned recipe)</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.967171</td>\n",
       "      <td>0.948036</td>\n",
       "      <td>0.936158</td>\n",
       "      <td>0.942059</td>\n",
       "      <td>0.992427</td>\n",
       "      <td>/workspace/ade-project/models/PubMedBERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BERT-base (tuned recipe)</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.929410</td>\n",
       "      <td>0.865932</td>\n",
       "      <td>0.890215</td>\n",
       "      <td>0.877905</td>\n",
       "      <td>0.973231</td>\n",
       "      <td>/workspace/ade-project/models/BERT-base</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model  ThresholdUsed  Ext_Accuracy  Ext_Precision  \\\n",
       "0     BioBERT (tuned recipe)           0.32      0.981800       0.961742   \n",
       "1  PubMedBERT (tuned recipe)           0.68      0.967171       0.948036   \n",
       "2   BERT-base (tuned recipe)           0.40      0.929410       0.865932   \n",
       "\n",
       "   Ext_Recall    Ext_F1   Ext_AUC                                Checkpoint  \n",
       "0    0.974940  0.968296  0.996484     /workspace/ade-project/models/BioBERT  \n",
       "1    0.936158  0.942059  0.992427  /workspace/ade-project/models/PubMedBERT  \n",
       "2    0.890215  0.877905  0.973231   /workspace/ade-project/models/BERT-base  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /workspace/ade-project/outputs/external_setfit_eval.csv\n",
      "Saved: /workspace/ade-project/outputs/external_setfit_eval.json\n"
     ]
    }
   ],
   "source": [
    "# ===== External evaluation on SetFit ADE (test split) =====\n",
    "import os, json, numpy as np, pandas as pd, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "\n",
    "# 1) Load SetFit ADE external test set\n",
    "ext = load_dataset(\"SetFit/ade_corpus_v2_classification\")\n",
    "external_test = ext[\"test\"]  # keep as-is; it already has columns: 'text', 'label'\n",
    "print(\"External test size:\", len(external_test))\n",
    "\n",
    "# 2) Helper to evaluate a saved HF checkpoint at a fixed probability threshold\n",
    "def eval_ckpt(ckpt_path: str, threshold: float, dataset, max_len=512):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tok = AutoTokenizer.from_pretrained(ckpt_path, use_fast=True, local_files_only=False)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(ckpt_path, local_files_only=False).to(device)\n",
    "\n",
    "    def tok_fn(ex): \n",
    "        return tok(ex[\"text\"], truncation=True, max_length=max_len)\n",
    "    dset_tok = dataset.map(tok_fn, batched=True)\n",
    "\n",
    "    tr = Trainer(model=mdl, tokenizer=tok)\n",
    "    pred = tr.predict(dset_tok)\n",
    "    logits = pred.predictions\n",
    "    y_true = np.array(pred.label_ids)\n",
    "\n",
    "    # P(class=1) from 2-logit difference\n",
    "    prob1 = 1/(1+np.exp(-(logits[:,1]-logits[:,0])))\n",
    "    y_hat = (prob1 >= threshold).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_hat, average=\"binary\", zero_division=0)\n",
    "    try: auc = roc_auc_score(y_true, prob1)\n",
    "    except: auc = float(\"nan\")\n",
    "    cm  = confusion_matrix(y_true, y_hat).tolist()\n",
    "    return {\"Accuracy\":acc, \"Precision\":pr, \"Recall\":rc, \"F1\":f1, \"AUC\":auc, \"CM\":cm}\n",
    "\n",
    "# 3) Load your tuned models + thresholds from the JSON you saved earlier\n",
    "CHOICES_JSON = \"/workspace/ade-project/outputs/comparison_tuned_recipe.json\"\n",
    "rows = json.load(open(CHOICES_JSON))\n",
    "\n",
    "# keep only transformer rows (skip TF-IDF because there’s no HF checkpoint path)\n",
    "models = [r for r in rows if \"Checkpoint\" in r and r[\"Checkpoint\"] != \"(sklearn pipeline)\"]\n",
    "assert len(models) > 0, \"No transformer checkpoints found in your comparison JSON.\"\n",
    "\n",
    "# 4) Evaluate all models on SetFit test split\n",
    "ext_rows = []\n",
    "for m in models:\n",
    "    name = m[\"Model\"]\n",
    "    thr  = float(m[\"ThresholdUsed\"])\n",
    "    ckpt = m[\"Checkpoint\"]\n",
    "    print(f\"Scoring external SetFit — {name} @ thr={thr} …\")\n",
    "    met = eval_ckpt(ckpt, thr, external_test)\n",
    "    ext_rows.append({\n",
    "        \"Model\": name,\n",
    "        \"ThresholdUsed\": thr,\n",
    "        \"Ext_Accuracy\": met[\"Accuracy\"],\n",
    "        \"Ext_Precision\": met[\"Precision\"],\n",
    "        \"Ext_Recall\": met[\"Recall\"],\n",
    "        \"Ext_F1\": met[\"F1\"],\n",
    "        \"Ext_AUC\": met[\"AUC\"],\n",
    "        \"Checkpoint\": ckpt\n",
    "    })\n",
    "\n",
    "df_ext = pd.DataFrame(ext_rows).sort_values(\"Ext_F1\", ascending=False).reset_index(drop=True)\n",
    "display(df_ext)\n",
    "\n",
    "# 5) Save\n",
    "OUT = \"/workspace/ade-project/outputs\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "CSV = f\"{OUT}/external_setfit_eval.csv\"\n",
    "JSON = f\"{OUT}/external_setfit_eval.json\"\n",
    "df_ext.to_csv(CSV, index=False)\n",
    "json.dump(ext_rows, open(JSON, \"w\"), indent=2)\n",
    "print(\"Saved:\", CSV)\n",
    "print(\"Saved:\", JSON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "096bd8c2-6367-4336-ab74-03e4b6845aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'label_text'],\n",
      "        num_rows: 17637\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'label_text'],\n",
      "        num_rows: 5879\n",
      "    })\n",
      "})\n",
      "=== Random test examples ===\n",
      "[ADE] CONCLUSIONS: Clinicians should be aware of a risk of serotonin syndrome with serious extrapyramidal reactions in patients receiving sertraline or venlafaxine when metoclopramide is coadministered even in a single, conventional dose.\n",
      "\n",
      "[no_ADE] He was referred after a percutaneous liver biopsy which revealed a moderately differentiated HCC.\n",
      "\n",
      "[no_ADE] Measurements of the peripheral arterial circulation were made using the Doppler ultrasonic velocity detector.\n",
      "\n",
      "=== One ADE example ===\n",
      "[ADE] The patient was given methimazole instead of propylthiouracil but, 10 weeks later, agranulocytosis again occurred.\n",
      "\n",
      "=== One no-ADE example ===\n",
      "[no_ADE] The use of somatostatin analog in gastroenteropancreatic tumors other than carcinoid.\n",
      "\n",
      "train label counts: {'no_ADE': 12492, 'ADE': 5145}\n",
      "test label counts: {'no_ADE': 4203, 'ADE': 1676}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Load\n",
    "ds = load_dataset(\"SetFit/ade_corpus_v2_classification\")\n",
    "print(ds)  # shows splits and sizes\n",
    "\n",
    "# Label names (0 = no ADE, 1 = ADE)\n",
    "label_names = {0: \"no_ADE\", 1: \"ADE\"}\n",
    "\n",
    "# Helper to pretty-print an example\n",
    "def show(ex):\n",
    "    print(f\"[{label_names[int(ex['label'])]}] {ex['text']}\\n\")\n",
    "\n",
    "# Look at 3 random test examples\n",
    "print(\"=== Random test examples ===\")\n",
    "for ex in ds[\"test\"].shuffle(seed=42).select(range(3)):\n",
    "    show(ex)\n",
    "\n",
    "# Look at one positive (ADE) and one negative (no_ADE) from test\n",
    "pos = next(ex for ex in ds[\"test\"] if ex[\"label\"] == 1)\n",
    "neg = next(ex for ex in ds[\"test\"] if ex[\"label\"] == 0)\n",
    "\n",
    "print(\"=== One ADE example ===\")\n",
    "show(pos)\n",
    "print(\"=== One no-ADE example ===\")\n",
    "show(neg)\n",
    "\n",
    "# Quick label distribution (train/test)\n",
    "for split in [\"train\", \"test\"]:\n",
    "    counts = {0:0, 1:0}\n",
    "    for y in ds[split][\"label\"]:\n",
    "        counts[int(y)] += 1\n",
    "    print(f\"{split} label counts:\", {label_names[k]: v for k,v in counts.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9ff71e-530a-4aba-8c20-a648aa739485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab931c1b-e62c-4878-8eca-1f1696f83072",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e5a4a-4505-4f89-a645-d5f0e149dead",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200dc2f9-eb54-4820-8b05-50abb3de5bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa030756-80de-4a48-b53d-f4c3b419b701",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc866b6-716d-4072-8faa-0e56ad4ff813",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
