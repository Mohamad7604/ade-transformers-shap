# ADE Transformers with SHAP

Benchmarking **biomedical transformers** for adverse drug event (ADE) detection on **ADE Corpus v2**, with **post-hoc SHAP** explanations and **external validation** on the SetFit ADE split.

> Notebooks only (no datasets). Reproducible end-to-end: preprocessing â†’ training â†’ threshold tuning â†’ evaluation â†’ SHAP.

---

## ğŸ“¦ Repository contents
notebooks/
01_finetune_ade_v2.ipynb # full pipeline: preprocessing â†’ training â†’ tuning â†’ SHAP
02_external_eval_setfit.ipynb # external evaluation on SetFit ADE (no retraining; fixed thresholds)
requirements.txt
LICENSE
README.md


---

## â¬‡ï¸ Get the notebooks
**Option A â€” Download ZIP**
1. Click the green **Code** button â†’ **Download ZIP**.
2. Unzip locally.

**Option B â€” Clone**
```bash
git clone https://github.com/<your-username>/ade-transformers-shap.git
cd ade-transformers-shap

ğŸ› ï¸ Setup

Create a fresh Python 3.10 environment and install dependencies:

# with conda (example)
conda create -n ade-ade python=3.10 -y
conda activate ade-ade

# install packages
pip install -r requirements.txt


requirements.txt (pinned): torch, transformers, datasets, evaluate, scikit-learn, pandas, numpy, matplotlib, seaborn, shap.

ğŸ§ª Data

ADE Corpus v2 (train/val/test; abstracts).

SetFit ADE split (external evaluation).

Datasets are NOT included. Obtain ADE v2 from its original source; load SetFit ADE via Hugging Face Datasets. Configure paths/loaders in the first cells of each notebook.

â–¶ï¸ Run

Launch Jupyter:

jupyter notebook


Open notebooks/01_finetune_ade_v2.ipynb

Preprocess â†’ train BioBERT / PubMedBERT / BERT-base + TF-IDF baselines

Tune decision thresholds on validation (max F1)

Export metrics/plots/SHAP examples to results/ (paths shown in the notebook)

Open notebooks/02_external_eval_setfit.ipynb

Loads the fixed thresholds from step 2

Evaluates on SetFit ADE without retraining

ğŸ“Š Results (summary)

Numbers may vary slightly; these are the reference values from the manuscript.

ADE v2 (held-out test; thresholds tuned on validation)

BioBERT â€” F1 0.8884, AUC 0.9838

PubMedBERT â€” F1 0.8836, AUC 0.9854

BERT-base â€” F1 0.8331, AUC 0.9677

TF-IDF + LinearSVM â€” F1 0.7283, AUC 0.9293

SetFit ADE (external; same thresholds, no retraining)

BioBERT â€” F1 0.9683, AUC 0.9965

PubMedBERT â€” F1 0.9421, AUC 0.9924

BERT-base â€” F1 0.8779, AUC 0.9732

SHAP explanations
Token-level SHAP highlights align with clinical intuition: ADE lexemes and causal markers (e.g., rash, nausea, after) increase 
ğ‘
(
ğ´
ğ·
ğ¸
)
p(ADE); negations/ameliorative phrases (no adverse effects, resolved) decrease it.

ğŸ” Reproducibility

Leak-free 80/10/10 splits (hash-checked)

Fixed random seeds

Thresholds tuned on validation and frozen for test & external eval

Notebooks save metrics, confusion matrices, ROC/PR curves, and SHAP examples

ğŸ§‘â€âš–ï¸ License

Code released under the MIT License (see LICENSE).

ğŸ“£ Cite

If you use this repository, please cite:
El Husseiny M., Ishak D. Benchmarking biomedical transformers for ADE detection with SHAP explanations. 2025. (manuscript)

âœ‰ï¸ Contact

Questions or issues? Open a GitHub Issue or email your.email@domain
.

::contentReference[oaicite:0]{index=0}
